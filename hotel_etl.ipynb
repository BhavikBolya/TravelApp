{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, functions, types\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.conf import SparkConf\n",
    "from geopy.geocoders import Nominatim\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SparkConf().set('spark.rpc.message.maxSize', '50')\n",
    "sc=pyspark.SparkContext(appName=\"project\")\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading hotel dataset scraped from TripAdvisor\n",
    "\n",
    "# h_df = pd.read_json('Testing/Project-Dataset/hotel_info.json')\n",
    "# h1_df = spark.createDataFrame(h_df)\n",
    "h1_df = spark.read.format('org.apache.spark.sql.json') \\\n",
    "        .load(\"Testing/Project-Dataset/hotel_info2.json\").cache()\n",
    "h1_df.createOrReplaceTempView('h1_df')\n",
    "## Removing duplicates from the hotel dataset\n",
    "\n",
    "temp=spark.sql(\"SELECT df.id FROM (SELECT id, COUNT(*) as tot_count FROM h1_df GROUP BY id ORDER BY tot_count DESC) df WHERE df.tot_count>1\")\n",
    "temp.createOrReplaceTempView('temp')\n",
    "del_dup = spark.sql(\"SELECT h1_df.* FROM h1_df LEFT JOIN temp ON h1_df.id == temp.id WHERE temp.id IS NULL\").cache()\n",
    "del_dup.createOrReplaceTempView('del_dup')\n",
    "\n",
    "del_dup.write.mode(\"overwrite\").json(\"Testing/Project-Dataset/del_dup/city1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Getting city using coordinates\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"new_recomm\", timeout=None)\n",
    "\n",
    "# def get_cname(x):\n",
    "#     if 'nil' in x[1:-1]:\n",
    "#         return \"None\"\n",
    "#     else:\n",
    "#         location = geolocator.reverse(x[1:-1], timeout=None)\n",
    "#         if 'city' in location.raw[\"address\"]:\n",
    "#             return location.raw[\"address\"][\"city\"]\n",
    "#         elif 'town' in location.raw[\"address\"]:\n",
    "#             return location.raw[\"address\"][\"town\"]\n",
    "#         else:\n",
    "#             return \"None\"\n",
    "    \n",
    "# get_city = functions.udf(lambda a:get_cname(a),types.StringType())\n",
    "\n",
    "city_df = del_dup.withColumn(\"location\", del_dup.location)\n",
    "\n",
    "## Saving etled dataset\n",
    "\n",
    "city_df.createOrReplaceTempView('del_dup')\n",
    "city_df.write.mode(\"overwrite\").json(\"Testing/Project-Dataset/del_dup\")\n",
    "# city_df.write.json('Testing/Project-Dataset/del_dup')\n",
    "# with open(\"Testing/Project-Dataset/del_dup/city1.json\", \"w+\") as output_file:\n",
    "#     output_file.write(city_df.toJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'explode(del_dup.`amenities`)' due to data type mismatch: input to function explode should be array or map type, not struct<0:array<string>,1:array<string>,10:array<string>,11:array<string>,12:array<string>,13:array<string>,14:array<string>,15:array<string>,16:array<string>,17:array<string>,18:array<string>,19:array<string>,2:array<string>,20:array<string>,21:array<string>,22:array<string>,23:array<string>,24:array<string>,25:array<string>,26:array<string>,27:array<string>,28:array<string>,29:array<string>,3:array<string>,30:array<string>,31:array<string>,32:array<string>,33:array<string>,34:array<string>,35:array<string>,36:array<string>,37:array<string>,38:array<string>,39:array<string>,4:array<string>,40:array<string>,41:array<string>,42:array<string>,43:array<string>,44:array<string>,45:array<string>,46:array<string>,47:array<string>,48:array<string>,49:array<string>,5:array<string>,6:array<string>,7:array<string>,8:array<string>,9:array<string>>; line 1 pos 10;\n'Project [id#423, explode(amenities#418) AS amenities#1514]\n+- SubqueryAlias del_dup\n   +- Project [address#417, amenities#418, country#419, hotel_experience#420, hotel_name#421, hotel_rating#422, id#423, location#424 AS location#1135, price#425]\n      +- Project [address#417, amenities#418, country#419, hotel_experience#420, hotel_name#421, hotel_rating#422, id#423, location#424, price#425]\n         +- Filter isnull(id#445)\n            +- Join LeftOuter, (id#423 = id#445)\n               :- SubqueryAlias h1_df\n               :  +- Relation[address#417,amenities#418,country#419,hotel_experience#420,hotel_name#421,hotel_rating#422,id#423,location#424,price#425] json\n               +- SubqueryAlias temp\n                  +- Project [id#445]\n                     +- Filter (tot_count#435L > cast(1 as bigint))\n                        +- SubqueryAlias df\n                           +- Sort [tot_count#435L DESC NULLS LAST], true\n                              +- Aggregate [id#445], [id#445, count(1) AS tot_count#435L]\n                                 +- SubqueryAlias h1_df\n                                    +- Relation[address#439,amenities#440,country#441,hotel_experience#442,hotel_name#443,hotel_rating#444,id#445,location#446,price#447] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1436/24335087.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Explode amenities to make predictions based on length of amentities provided by user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnewh_df\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT id,explode(amenities) as amenities FROM del_dup\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m##  Removing punctuations from amenities column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\context.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \"\"\"\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'explode(del_dup.`amenities`)' due to data type mismatch: input to function explode should be array or map type, not struct<0:array<string>,1:array<string>,10:array<string>,11:array<string>,12:array<string>,13:array<string>,14:array<string>,15:array<string>,16:array<string>,17:array<string>,18:array<string>,19:array<string>,2:array<string>,20:array<string>,21:array<string>,22:array<string>,23:array<string>,24:array<string>,25:array<string>,26:array<string>,27:array<string>,28:array<string>,29:array<string>,3:array<string>,30:array<string>,31:array<string>,32:array<string>,33:array<string>,34:array<string>,35:array<string>,36:array<string>,37:array<string>,38:array<string>,39:array<string>,4:array<string>,40:array<string>,41:array<string>,42:array<string>,43:array<string>,44:array<string>,45:array<string>,46:array<string>,47:array<string>,48:array<string>,49:array<string>,5:array<string>,6:array<string>,7:array<string>,8:array<string>,9:array<string>>; line 1 pos 10;\n'Project [id#423, explode(amenities#418) AS amenities#1514]\n+- SubqueryAlias del_dup\n   +- Project [address#417, amenities#418, country#419, hotel_experience#420, hotel_name#421, hotel_rating#422, id#423, location#424 AS location#1135, price#425]\n      +- Project [address#417, amenities#418, country#419, hotel_experience#420, hotel_name#421, hotel_rating#422, id#423, location#424, price#425]\n         +- Filter isnull(id#445)\n            +- Join LeftOuter, (id#423 = id#445)\n               :- SubqueryAlias h1_df\n               :  +- Relation[address#417,amenities#418,country#419,hotel_experience#420,hotel_name#421,hotel_rating#422,id#423,location#424,price#425] json\n               +- SubqueryAlias temp\n                  +- Project [id#445]\n                     +- Filter (tot_count#435L > cast(1 as bigint))\n                        +- SubqueryAlias df\n                           +- Sort [tot_count#435L DESC NULLS LAST], true\n                              +- Aggregate [id#445], [id#445, count(1) AS tot_count#435L]\n                                 +- SubqueryAlias h1_df\n                                    +- Relation[address#439,amenities#440,country#441,hotel_experience#442,hotel_name#443,hotel_rating#444,id#445,location#446,price#447] json\n"
     ]
    }
   ],
   "source": [
    "## Explode amenities to make predictions based on length of amentities provided by user\n",
    "\n",
    "newh_df  = spark.sql(\"SELECT id,explode(amenities) as amenities FROM del_dup\")\n",
    "\n",
    "##  Removing punctuations from amenities column\n",
    "\n",
    "strip_udf = functions.udf(lambda x: re.sub(r'[^\\w\\s]','',x), types.StringType())\n",
    "newh_df = newh_df.withColumn(\"amenities\", strip_udf(functions.col(\"amenities\")))\n",
    "newh_df.createOrReplaceTempView('newh_df')\n",
    "# newh_df.coalesce(4).write.json('Testing/Project-Dataset/newh_df',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('tripadvisor_hotel_output/reviews.json')\n",
    "\n",
    "df[\"att_id\"]=df.id.astype('category').cat.codes\n",
    "\n",
    "rev_df = spark.createDataFrame(df).cache()\n",
    "rev_df.createOrReplaceTempView('rev_df')\n",
    "\n",
    "rev_temp=spark.sql(\"SELECT df.id FROM (SELECT id, COUNT(*) as tot_count FROM rev_df GROUP BY id ORDER BY tot_count DESC) df WHERE df.tot_count>1\")\n",
    "rev_temp.createOrReplaceTempView('rev_temp')\n",
    "\n",
    "s_df = spark.sql(\"SELECT rev_df.* FROM rev_df LEFT JOIN rev_temp ON rev_df.id == rev_temp.id WHERE rev_temp.id IS NULL\")\n",
    "s_df.createOrReplaceTempView('s_df')\n",
    "\n",
    "\n",
    "## String Indexing user_name \n",
    "\n",
    "indexer = StringIndexer(inputCol=\"user_name\", outputCol=\"user_id\")\n",
    "indexed = indexer.fit(s_df).transform(s_df)\n",
    "u_id_df = indexed.withColumn(\"user_id\",indexed[\"user_id\"].cast(\"Int\")).cache()\n",
    "u_id_df.createOrReplaceTempView('u_id_df')\n",
    "u_id_df.coalesce(4).write.json('etl/u_id_df',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
