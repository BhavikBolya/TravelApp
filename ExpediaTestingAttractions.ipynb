{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries have been successfully imported.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "print('All required libraries have been successfully imported.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attraction urls file read successfully..\n",
      "Number of Attractions: 1391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('Testing/Project-Dataset/jaipur_attraction_urls.txt', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    attraction_full_urllist = list(reader)\n",
    "\n",
    "print('Attraction urls file read successfully..')\n",
    "print(f'Number of Attractions: {len(attraction_full_urllist)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391\n"
     ]
    }
   ],
   "source": [
    "new_l = []\n",
    "for iter_val in range(len(attraction_full_urllist)):\n",
    "    url = str(attraction_full_urllist[iter_val])[2:-2]\n",
    "    if 'agra' not in url and 'delhi' not in url:\n",
    "        new_l.append(url)\n",
    "print(len(new_l))\n",
    "with open(\"jaipur_category_urls.txt\", 'w') as output:\n",
    "    for row in new_l:\n",
    "        output.write(str(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "361\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1294\n",
      "1300\n",
      "1350\n",
      "Extracted info of 1385 attractions\n"
     ]
    }
   ],
   "source": [
    "attraction_info = []\n",
    "for iter_val in range(len(attraction_full_urllist)):\n",
    "    url = str(attraction_full_urllist[iter_val])[2:-2]\n",
    "\n",
    "    # Get the attraction page from url and create a beautiful soup instance\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    # BASIC INFORMATION\n",
    "    div1 = soup.find('div', class_ = 'uitk-flex uitk-flex-column uitk-layout-grid-item uitk-layout-grid-item-columnspan-small-12 uitk-layout-grid-item-columnspan-large-4 uitk-card all-r-padding-six')\n",
    "    try:\n",
    "        attraction_name = div1.find('h1').text\n",
    "    except:\n",
    "        print(iter_val+1)\n",
    "        continue\n",
    "    try:\n",
    "        attraction_rating = div1.find('div', class_ = 'uitk-text uitk-type-500 uitk-type-bold uitk-flex-align-self-center uitk-flex-item uitk-text-default-theme').text[0:-2]\n",
    "    except:\n",
    "        attraction_rating = ''\n",
    "    \n",
    "    try:\n",
    "        attraction_price = div1.find('span', class_ = 'uitk-lockup-price').text\n",
    "    except:\n",
    "        continue\n",
    "    # FEATURES\n",
    "    features_div = soup.find('div', class_ = 'uitk-layout-grid uitk-layout-grid-gap-one uitk-layout-grid-columns-12')\n",
    "    features_div = features_div.find_all('div', class_ = 'uitk-text uitk-type-300 uitk-spacing uitk-spacing-margin-inlinestart-one uitk-text-default-theme')\n",
    "\n",
    "    attraction_features = []\n",
    "\n",
    "    for feature in features_div:\n",
    "        attraction_features.append(feature.text)\n",
    "    \n",
    "    # LOCATION\n",
    "    location = soup.find('img', class_ = \"uitk-image-media uitk-card-roundcorner-all padding-bottom-zero\")\n",
    "\n",
    "    location_link = location.attrs['src']\n",
    "    coords = re.findall('%7C[0-9]+.[0-9]+,[0-9]+.[0-9]+', location_link).pop(1)\n",
    "    coords = coords[3:]\n",
    "    latitude, longitude = coords.split(\",\")\n",
    "\n",
    "    # CATEGORY\n",
    "    attraction_category = re.findall('.categories=\\w+', url)[0].replace('.categories=', '')\n",
    "\n",
    "    attraction_info.append([iter_val+1, attraction_name, attraction_category, attraction_rating, attraction_price, latitude, longitude, \"Jaipur\", \"India\"])\n",
    "    if iter_val%50 == 0:\n",
    "        print(iter_val)\n",
    "\n",
    "print(f\"Extracted info of {len(attraction_info)} attractions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(fields, rows, dirname, filename):\n",
    "    with open(dirname+filename, 'w', encoding='UTF8', newline='') as csvfile: \n",
    "        # creating a csv writer object\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fields) \n",
    "        csvwriter.writerows(rows)\n",
    "    print(f\"Saved csv with name {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved csv with name attraction_info.csv\n",
      "Hotel Dataframe updated..\n"
     ]
    }
   ],
   "source": [
    "fields = ['attraction_id', 'attraction_name','attraction_category', 'attraction_rating', 'attraction_price', 'latitude', 'longitude', 'city', 'country']\n",
    "create_csv(fields, attraction_info, \"Testing/Project-Dataset/\", \"attraction_info.csv\")\n",
    "print('Hotel Dataframe updated..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(csvFilePath, jsonFilePath):\n",
    "    \n",
    "    # create a dictionary\n",
    "    data = \"\"\n",
    "\n",
    "    # Open a csv reader called DictReader\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "        csvReader = csv.DictReader(csvf)\n",
    "\n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for rows in csvReader:\n",
    "\n",
    "            # Assuming a column named 'No' to\n",
    "            # be the primary key\n",
    "            \n",
    "            data = data+'\\n'\n",
    " \n",
    "    # Open a json writer, and use the json.dumps()\n",
    "    # function to dump data\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf:\n",
    "        jsonf.write(json.dumps(data ,indent=1))\n",
    "\n",
    "# Driver Code\n",
    " \n",
    "# Decide the two file paths according to your\n",
    "# computer system\n",
    "csvFilePath = r'Testing/Project-Dataset/hotel_info.csv'\n",
    "jsonFilePath = r'Testing/Project-Dataset/hotel_info3.json'\n",
    " \n",
    "# Call the make_json function\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaa29a38bd2b6d3dd4d71a3f1d87df91209d6946247085b4849f410a246e8dc0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
