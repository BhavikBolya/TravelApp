{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all module are loaded \n"
     ]
    }
   ],
   "source": [
    "# Spoofer code\n",
    "try:\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    from fp.fp import FreeProxy\n",
    "    from fake_useragent import UserAgent\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver import Chrome\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    import time\n",
    "    print('all module are loaded ')\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Error ->>>: {} \".format(e))\n",
    "\n",
    "\n",
    "class Spoofer(object):\n",
    "\n",
    "    def __init__(self, country_id=['US'], rand=True, anonym=True):\n",
    "        self.country_id = country_id\n",
    "        self.rand = rand\n",
    "        self.anonym = anonym\n",
    "        self.userAgent, self.ip = self.get()\n",
    "\n",
    "    def get(self):\n",
    "        ua = UserAgent()\n",
    "        proxy = FreeProxy(country_id=self.country_id, rand=self.rand, anonym=self.anonym).get()\n",
    "        ip = proxy.split(\"://\")[1]\n",
    "        return ua.random, ip\n",
    "\n",
    "\n",
    "class DriverOptions(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.options = Options()\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--start-maximized')\n",
    "        self.options.add_argument('--start-fullscreen')\n",
    "        self.options.add_argument('--single-process')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_argument(\"--incognito\")\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_experimental_option('useAutomationExtension', False)\n",
    "        self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        self.options.add_argument(\"disable-infobars\")\n",
    "\n",
    "        self.helperSpoofer = Spoofer()\n",
    "\n",
    "        self.options.add_argument('user-agent={}'.format(self.helperSpoofer.userAgent))\n",
    "        self.options.add_argument('--proxy-server=%s' % self.helperSpoofer.ip)\n",
    "\n",
    "\n",
    "class WebDriver(DriverOptions):\n",
    "\n",
    "    def __init__(self, path=''):\n",
    "        DriverOptions.__init__(self)\n",
    "        self.driver_instance = self.get_driver()\n",
    "\n",
    "    def get_driver(self):\n",
    "\n",
    "        print(\"\"\"\n",
    "        IP:{}\n",
    "        UserAgent: {}\n",
    "        \"\"\".format(self.helperSpoofer.ip, self.helperSpoofer.userAgent))\n",
    "\n",
    "        PROXY = self.helperSpoofer.ip\n",
    "        webdriver.DesiredCapabilities.CHROME['proxy'] = {\n",
    "            \"httpProxy\":PROXY,\n",
    "            \"ftpProxy\":PROXY,\n",
    "            \"sslProxy\":PROXY,\n",
    "            \"noProxy\":None,\n",
    "            \"proxyType\":\"MANUAL\",\n",
    "            \"autodetect\":False\n",
    "        }\n",
    "        webdriver.DesiredCapabilities.CHROME['acceptSslCerts'] = True\n",
    "\n",
    "        path = os.path.join(os.getcwd(), '/chromedriver.exe')\n",
    "\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\":\n",
    "                \"const newProto = navigator.__proto__;\"\n",
    "                \"delete newProto.webdriver;\"\n",
    "                \"navigator.__proto__ = newProto;\"\n",
    "        })\n",
    "\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a csv by giving the field name, its contents, the directory name and filename\n",
    "\n",
    "def create_csv(fields, rows, dirname, filename):\n",
    "    with open(dirname+filename, 'w', encoding='UTF8', newline='') as csvfile: \n",
    "        # creating a csv writer object\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fields) \n",
    "        csvwriter.writerows(rows)\n",
    "    print(f\"Saved csv with name {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        IP:20.47.108.204:8888\n",
      "        UserAgent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0\n",
      "        \n",
      "Driver Successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create anti bot detection web driver\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver = WebDriver()\n",
    "        break\n",
    "    except: # If Driver creation fails try again\n",
    "        print(\"Driver creation failed.....Trying again\")\n",
    "        continue\n",
    "print(\"Driver Successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract url of every attraction from the given page url\n",
    "def get_attr_urls(url):\n",
    "\n",
    "    # Create Driver instance and open the webpage through selenium\n",
    "    driverinstance = driver.driver_instance\n",
    "    driverinstance.get(url)\n",
    "\n",
    "    # Click on the \"SHOW MORE\" button 20 times to load enough hotels \n",
    "    for i in range(10):\n",
    "        sleep_time = 5 + 5*random.random()\n",
    "        time.sleep(sleep_time)\n",
    "        driverinstance.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        try:\n",
    "            button = driverinstance.find_element_by_xpath('//div[@class=\"uitk-spacing uitk-type-center uitk-spacing-margin-blockstart-two uitk-spacing-margin-blockend-four\"]/button')\n",
    "            button.click()\n",
    "        except:\n",
    "            print(\"Button not Found, ending search\")\n",
    "            break\n",
    "\n",
    "    # Create BeautifulSoup instance and search for each hotel on the page to get it's link\n",
    "    soup = str(BeautifulSoup(driverinstance.page_source, 'html.parser'))\n",
    "    soup = BeautifulSoup(soup, 'lxml')\n",
    "\n",
    "    # Getting the url of each attraction and returning it as a list\n",
    "    sec = soup.find('section', class_=\"uitk-layout-grid uitk-layout-grid-gap-three uitk-layout-grid-align-content-space-between uitk-layout-grid-columns-small-2 uitk-layout-grid-columns-medium-3 uitk-layout-grid-columns-large-3\") \n",
    "    card = sec.find_all('div' , attrs= {\"data-testid\":\"activity-tile-card\"})\n",
    "    attraction_url_list=[]\n",
    "    for i in card:\n",
    "        attr_url = \"https://www.expedia.com\"+i.find('a', class_ = \"uitk-card-link\").attrs['href']\n",
    "        attraction_url_list.append(attr_url)\n",
    "    return attraction_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "Button not Found, ending search\n",
      "1847\n"
     ]
    }
   ],
   "source": [
    "# Open the text file containing links to page with attractions of each category\n",
    "with open('Testing/Project-Dataset/jaipur_category_urls.txt', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    category_full_urllist = list(reader)\n",
    "length_df = int(len(category_full_urllist))\n",
    "\n",
    "# Iterating through all the category links to get links of each attraction\n",
    "attraction_urls = []\n",
    "for iter_val in range(length_df):\n",
    "    url = str(category_full_urllist[iter_val])[2:-2]\n",
    "    attraction_urls+= get_attr_urls(url)\n",
    "print(len(attraction_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attractions extracted: 1722\n"
     ]
    }
   ],
   "source": [
    "# Removing Duplicate attractions\n",
    "\n",
    "res = []\n",
    "for i in attraction_urls:\n",
    "    if i not in res:\n",
    "        res.append(i)\n",
    "print(f'Number of Attractions extracted: {len(res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attraction links saved in text file successfully\n"
     ]
    }
   ],
   "source": [
    "# Save all the links in a text file\n",
    "\n",
    "with open(\"Testing/Project-Dataset/jaipur_attraction_urls.txt\", 'w') as output:\n",
    "    for row in res:\n",
    "        output.write(str(row) + '\\n')\n",
    "print(\"Attraction links saved in text file successfully\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ac516125b3ee16d4eaa41f008fade7bad50b808bd1b6d74d2e8ae0015ba9066"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
